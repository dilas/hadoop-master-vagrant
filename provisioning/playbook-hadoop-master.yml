---
- hosts: hadoop-master  
  gather_facts: yes
  remote_user: root
  
  vars:
    install_directory_cache: /home/vagrant/install
    hadoop_home: /opt/hadoop
    spark_home: /opt/spark
  
  tasks:
  
  - name: change machine hostname
    hostname: name=hadoop-master

  - name: Generate /etc/hosts file
    template: src=hadoop-master/hosts.j2 dest=/etc/hosts

  - name: update apt cache
    apt: update_cache=yes cache_valid_time=3600

  - name: install required packages
    apt: name={{item}} state=present
    with_items:
      - default-jdk
      - zip
  
  - name: create hadoop group
    group: name=hadoop state=present

  - name: create hadoop user
    user: name=hadoop password={{'hadoop' | password_hash('sha512', 'salt')}} group=hadoop shell=/bin/bash state=present

  - name: create install directory cache
    file: path={{install_directory_cache}} state=directory

  - name: download apache hadoop release
    get_url:
      url: http://www-us.apache.org/dist/hadoop/common/hadoop-2.6.5/hadoop-2.6.5.tar.gz
      dest: "{{install_directory_cache}}/hadoop-2.6.5.tar.gz"
      checksum: sha256:001ad18d4b6d0fe542b15ddadba2d092bc97df1c4d2d797381c8d12887691898
                       
  - name: extract apache hadoop
    unarchive:
      src: "{{install_directory_cache}}/hadoop-2.6.5.tar.gz"
      dest: /opt
      remote_src: yes
      owner: hadoop
      group: hadoop
      creates: /opt/hadoop-2.6.5

  - name: symlink hadoop install directory
    file: src=/opt/hadoop-2.6.5 path={{hadoop_home}} state=link
  
  - name: create .ssh directory
    file: path=/home/hadoop/.ssh state=directory mode=0700 owner=hadoop group=hadoop

  - name: copy hadoop ssh config and keys
    copy: src={{item.src}} dest={{item.dst}} mode={{item.mode}} owner=hadoop group=hadoop
    with_items:
      - { src: 'hadoop-ssh-config/id_rsa', dst: '/home/hadoop/.ssh/id_rsa', mode: '0600' }
      - { src: 'hadoop-ssh-config/id_rsa.pub', dst: '/home/hadoop/.ssh/id_rsa.pub', mode: '0644' }
      - { src: 'hadoop-ssh-config/ssh_config', dst: '/home/hadoop/.ssh/config', mode: '0600' }

  - name: set up authorized_keys for the hadoop user
    authorized_key: user=hadoop key="{{item}}"
    with_file:
      - hadoop-ssh-config/id_rsa.pub

  - name: copy hadoop config files
    copy: src={{item}} dest="{{hadoop_home}}/etc/hadoop" owner=hadoop group=hadoop mode=0664
    with_fileglob:
      - hadoop-conf/*

  - name: create HDFS storage directories
    file: path=/home/hadoop/hdfs/{{item}} state=directory owner=hadoop group=hadoop
    with_items:
      - namenode
      - datanode

  - name: add JAVA_HOME environment variable globally
    lineinfile: dest=/etc/environment state=present insertbefore=BOF regexp='^JAVA_HOME' line='JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64'
    
  - name: add HADOOP_HOME environment variable globally
    lineinfile: dest=/etc/environment state=present insertafter='^JAVA_HOME' regexp='^HADOOP_HOME' line='HADOOP_HOME={{hadoop_home}}'

  - name: add HADOOP_CONF_DIR environment variable globally
    lineinfile: dest=/etc/environment state=present insertafter='^HADOOP_HOME' regexp='^HADOOP_CONF_DIR' line='HADOOP_CONF_DIR={{hadoop_home}}/etc/hadoop'

  - name: add HADOOP_HOME/bin into PATH environment variable globally
    lineinfile: >
     dest=/etc/environment
     state=present
     backrefs=yes
     regexp='PATH=(["]*)((?!.*?{{hadoop_home}}/bin).*?)(["]*)$'
     line="PATH=\1\2:{{hadoop_home}}/bin\3"    

  - name: add HADOOP_HOME/sbin into PATH environment variable globally
    lineinfile: >
     dest=/etc/environment
     state=present
     backrefs=yes
     regexp='PATH=(["]*)((?!.*?{{hadoop_home}}/sbin).*?)(["]*)$'
     line="PATH=\1\2:{{hadoop_home}}/sbin\3"    

  - name: format HDFS filesystem
    command: "{{hadoop_home}}/bin/hdfs namenode -format creates=/home/hadoop/hdfs/namenode/current"
    become: true
    become_user: hadoop

  - name: download apache spark release
    get_url:
      url: http://d3kbcqa49mib13.cloudfront.net/spark-2.0.2-bin-hadoop2.6.tgz
      dest: "{{install_directory_cache}}/spark-2.0.2-bin-hadoop2.6.tgz"
      checksum: md5:ec5cc551d93b455c1f3da878dcc26330

  - name: extract apache hadoop
    unarchive:
      src: "{{install_directory_cache}}/spark-2.0.2-bin-hadoop2.6.tgz"
      dest: /opt
      remote_src: yes
      owner: hadoop
      group: hadoop
      creates: /opt/spark-2.0.2-bin-hadoop2.6

  - name: symlink spark install directory
    file: src=/opt/spark-2.0.2-bin-hadoop2.6 path={{spark_home}} state=link

  - name: add SPARK_HOME environment variable globally
    lineinfile: dest=/etc/environment state=present insertafter='^HADOOP_CONF_DIR' regexp='^SPARK_HOME' line='SPARK_HOME={{spark_home}}'
    
  - name: add SPARK_HOME/bin into PATH environment variable globally
    lineinfile: >
     dest=/etc/environment
     state=present
     backrefs=yes
     regexp='PATH=(["]*)((?!.*?{{spark_home}}/bin).*?)(["]*)$'
     line="PATH=\1\2:{{spark_home}}/bin\3"    

  - name: install sbt build tool
    apt: deb=https://dl.bintray.com/sbt/debian/sbt-0.13.13.deb state=present